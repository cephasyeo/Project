{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Project Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function that estimates the transition parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6480490669450607\n",
      "0.000787675624187137\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "we need 2 things:\n",
    "every tag pair for consecutive words\n",
    "every tag\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# helper to read data by sentences\n",
    "def read_sentences(file_path):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                word, tag = line.split()\n",
    "                sentence.append(tag)\n",
    "            else:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# function to estimate transition parameters\n",
    "def estimate_transition_parameters(sentences):\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tags = ['START'] + sentence + ['STOP']\n",
    "        for i in range(1, len(tags)):\n",
    "            prev_tag = tags[i - 1]\n",
    "            curr_tag = tags[i]\n",
    "            transition_counts[prev_tag][curr_tag] += 1\n",
    "            tag_counts[prev_tag] += 1  # count only as previous tag\n",
    "\n",
    "    transition_probs = {}\n",
    "\n",
    "    for prev_tag in transition_counts:\n",
    "        transition_probs[prev_tag] = {}\n",
    "        for curr_tag in transition_counts[prev_tag]:\n",
    "            transition_probs[prev_tag][curr_tag] = transition_counts[prev_tag][curr_tag] / tag_counts[prev_tag]\n",
    "\n",
    "    return transition_probs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sentences = read_sentences(\"EN/train\")\n",
    "    transition_probs = estimate_transition_parameters(sentences)\n",
    "\n",
    "    # eg q(B-NP | START)\n",
    "    print(transition_probs[\"START\"].get(\"B-NP\", 0))\n",
    "\n",
    "    # eg q(STOP | I-NP)\n",
    "    print(transition_probs[\"I-NP\"].get(\"STOP\", 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Viterbi algorithm\n",
    "\n",
    "THIS IS WRONG AND INCOMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "given a sequence of words x1,...,xn, find the most likely tag sequence y1*,....,yn* using the Viterbi algo\n",
    "we need to maintain 2 things:\n",
    "pi[i][tag]: max log-prob of best path to position i ending in tag\n",
    "bp[i][tag]: best previous tag to reach tag at position i\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "def viterbi_decode(dev_path, output_path, tag_list, emission_probs, transition_probs, known_words):\n",
    "    \"\"\"\n",
    "    function applies viterbi algo to each sentence in the dev set and outputs best tag sequence\n",
    "    \"\"\"\n",
    "    # first, lets collect lines from dev.in and group them into sentences\n",
    "    with open(dev_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        word = line.strip()\n",
    "        if word == '':\n",
    "            if sentence:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            sentence.append(word)\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    with open(output_path, 'w') as f_out:\n",
    "        for sentence in sentences:\n",
    "            n = len(sentence)\n",
    "            pi = [{} for _ in range(n + 1)] # pi[i][tag] stores the best log-prob for reaching tag at position i\n",
    "            bp = [{} for _ in range(n + 1)] # bp[i][tag] stores the backpointer - best previous tag\n",
    "\n",
    "            pi[0][\"START\"] = 0.0\n",
    "\n",
    "            for i in range(1, n + 1):\n",
    "                word = sentence[i - 1]\n",
    "                word_key = word if word in known_words else \"#UNK#\" # if word not in training set, replace with #UNK#\n",
    "\n",
    "                for curr_tag in tag_list:\n",
    "                    if word_key not in emission_probs[curr_tag]:\n",
    "                        continue\n",
    "\n",
    "                    best_score = float(\"-inf\")\n",
    "                    best_prev_tag = None\n",
    "\n",
    "                    for prev_tag in pi[i - 1]:\n",
    "                        if curr_tag not in transition_probs.get(prev_tag, {}):\n",
    "                            continue\n",
    "                        \n",
    "                        # calc score for log-prob for best path to curr_tag at position i\n",
    "                        score = (\n",
    "                            pi[i - 1][prev_tag]\n",
    "                            + math.log(transition_probs[prev_tag][curr_tag])\n",
    "                            + math.log(emission_probs[curr_tag][word_key])\n",
    "                        )\n",
    "\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_prev_tag = prev_tag\n",
    "\n",
    "                    # update dynamic programming table & backpointer\n",
    "                    if best_prev_tag is not None:\n",
    "                        pi[i][curr_tag] = best_score\n",
    "                        bp[i][curr_tag] = best_prev_tag\n",
    "\n",
    "            # termination: pick best final tag\n",
    "            best_final_score = float(\"-inf\")\n",
    "            best_final_tag = None\n",
    "\n",
    "            for tag in pi[n]:\n",
    "                if \"STOP\" in transition_probs.get(tag, {}):\n",
    "                    score = pi[n][tag] + math.log(transition_probs[tag][\"STOP\"])\n",
    "                    if score > best_final_score:\n",
    "                        best_final_score = score\n",
    "                        best_final_tag = tag\n",
    "\n",
    "            # Fallback if no STOP transition found\n",
    "            if best_final_tag is None:\n",
    "                best_final_tag = max(pi[n], key=pi[n].get)  # use tag with highest score at final position\n",
    "\n",
    "            # backtracking: get full tag sequence\n",
    "            tags = [best_final_tag]\n",
    "            for i in range(n, 1, -1):\n",
    "                tags.insert(0, bp[i][tags[0]])\n",
    "\n",
    "            for word, tag in zip(sentence, tags):\n",
    "                f_out.write(f\"{word} {tag}\\n\")\n",
    "            f_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS THE FUNCTIONS FROM PART 1\n",
    "\n",
    "# function to read data\n",
    "def read_training_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip(): # not an empty line\n",
    "                word, tag = line.strip().split()\n",
    "                data.append((word, tag))\n",
    "    return data\n",
    "\n",
    "# function to estimate emission parameters\n",
    "def estimate_emission_parameters(data):\n",
    "    count_y = defaultdict(int) # count how many times y appears overall\n",
    "    count_y_to_x = defaultdict(lambda: defaultdict(int))  # count how many times word x is tagged with y\n",
    "    \n",
    "    for word, tag in data:\n",
    "        count_y[tag] += 1\n",
    "        count_y_to_x[tag][word] += 1\n",
    "\n",
    "    emission_probs = {}  # e(x|y)\n",
    "\n",
    "    for tag in count_y_to_x:\n",
    "        emission_probs[tag] = {}\n",
    "        for word in count_y_to_x[tag]:\n",
    "            emission_probs[tag][word] = count_y_to_x[tag][word] / count_y[tag]\n",
    "    \n",
    "    return emission_probs\n",
    "\n",
    "def replace_rare_words(data, word_counts, k=3):\n",
    "    new_data = []\n",
    "    for word, tag in data:\n",
    "        if word_counts[word] < k:\n",
    "            new_data.append(('#UNK#', tag))\n",
    "        else:\n",
    "            new_data.append((word, tag))\n",
    "    return new_data\n",
    "\n",
    "# function to count word frequencies\n",
    "def count_word_frequencies(data):\n",
    "    word_counts = defaultdict(int)\n",
    "    for word, _ in data:\n",
    "        word_counts[word] += 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m tag_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(emission_probs\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Run Viterbi\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mviterbi_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memission_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown_words\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 83\u001b[0m, in \u001b[0;36mviterbi_decode\u001b[1;34m(dev_path, output_path, tag_list, emission_probs, transition_probs, known_words)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Fallback if no STOP transition found\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_final_tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     best_final_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# use tag with highest score at final position\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# backtracking: get full tag sequence\u001b[39;00m\n\u001b[0;32m     86\u001b[0m tags \u001b[38;5;241m=\u001b[39m [best_final_tag]\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = \"EN/train\"\n",
    "    dev_path = \"EN/dev.in\"\n",
    "    output_path = \"EN/dev.p2.out\"\n",
    "\n",
    "    # Prepare emission\n",
    "    data = read_training_data(train_path)\n",
    "    word_counts = count_word_frequencies(data)\n",
    "    smoothed_data = replace_rare_words(data, word_counts, k=3)\n",
    "    emission_probs = estimate_emission_parameters(smoothed_data)\n",
    "    known_words = set(word for word, _ in smoothed_data)\n",
    "\n",
    "    # Prepare transitions\n",
    "    sentences = read_sentences(train_path)\n",
    "    transition_probs = estimate_transition_parameters(sentences)\n",
    "\n",
    "    # Get list of possible tags\n",
    "    tag_list = list(emission_probs.keys())\n",
    "\n",
    "    # Run Viterbi\n",
    "    viterbi_decode(dev_path, output_path, tag_list, emission_probs, transition_probs, known_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
