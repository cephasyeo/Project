{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Project Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function that estimates the emission parameters from the training set using MLE (maximum likelihood estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1639572983828348\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "each line in the training file contains a word and a tag, seperated by a tab.\n",
    "sentences are separated by empty lines\n",
    "\n",
    "to estimate the emission parameters from the training set, we need 2 things:\n",
    "1. a count of how many times each word appears with a tag\n",
    "2. a count of how many times each tag appears\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# function to read data\n",
    "def read_training_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip(): # not an empty line\n",
    "                word, tag = line.strip().split()\n",
    "                data.append((word, tag))\n",
    "    return data\n",
    "\n",
    "# function to estimate emission parameters\n",
    "def estimate_emission_parameters(data):\n",
    "    count_y = defaultdict(int) # count how many times y appears overall\n",
    "    count_y_to_x = defaultdict(lambda: defaultdict(int))  # count how many times word x is tagged with y\n",
    "    \n",
    "    for word, tag in data:\n",
    "        count_y[tag] += 1\n",
    "        count_y_to_x[tag][word] += 1\n",
    "\n",
    "    emission_probs = {}  # e(x|y)\n",
    "\n",
    "    for tag in count_y_to_x:\n",
    "        emission_probs[tag] = {}\n",
    "        for word in count_y_to_x[tag]:\n",
    "            emission_probs[tag][word] = count_y_to_x[tag][word] / count_y[tag]\n",
    "    \n",
    "    return emission_probs\n",
    "\n",
    "# example usage\n",
    "# e(\"the\" | \"B-NP\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    training_data = read_training_data(\"EN/train\")\n",
    "    emission_probs = estimate_emission_parameters(training_data)\n",
    "\n",
    "    # print emission probability for example\n",
    "    print(emission_probs[\"B-NP\"].get(\"the\", 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Unknown Words with Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0825071345523729\n"
     ]
    }
   ],
   "source": [
    "# function to replace words with count < k with '#UNK'\n",
    "def replace_rare_words(data, word_counts, k=3):\n",
    "    new_data = []\n",
    "    for word, tag in data:\n",
    "        if word_counts[word] < k:\n",
    "            new_data.append(('#UNK#', tag))\n",
    "        else:\n",
    "            new_data.append((word, tag))\n",
    "    return new_data\n",
    "\n",
    "# function to count word frequencies\n",
    "def count_word_frequencies(data):\n",
    "    word_counts = defaultdict(int)\n",
    "    for word, _ in data:\n",
    "        word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "# example usage\n",
    "# e(\"#UNK#\" | \"B-NP\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    training_data = read_training_data(\"EN/train\")\n",
    "    word_counts = count_word_frequencies(training_data)\n",
    "    smoothed_data = replace_rare_words(training_data, word_counts, k=3)\n",
    "\n",
    "    emission_probs = estimate_emission_parameters(smoothed_data)\n",
    "\n",
    "    print(emission_probs.get(\"B-NP\", {}).get(\"#UNK#\", 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Baseline Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word, pick the tag y that has the highest emission probability e(x|y)\n",
    "\n",
    "\"\"\"\n",
    "for each word in the dev set:\n",
    "if its in the training vocab, find the tag with the highest emission probability\n",
    "if not, replace with #UNK#, then do the same.\n",
    "write predicted tags to a new file\n",
    "\"\"\"\n",
    "\n",
    "def baseline_tagger(dev_path, output_path, emission_probs, known_words):\n",
    "    with open(dev_path, 'r') as f_in, open(output_path, 'w') as f_out:\n",
    "        for line in f_in:\n",
    "            word = line.strip()\n",
    "            if not word:\n",
    "                f_out.write('\\n')\n",
    "                continue\n",
    "\n",
    "            # Use word if seen, else use #UNK#\n",
    "            word_key = word if word in known_words else '#UNK#'\n",
    "\n",
    "            best_tag = None\n",
    "            best_score = 0\n",
    "\n",
    "            for tag in emission_probs:\n",
    "                prob = emission_probs[tag].get(word_key, 0)\n",
    "                if prob > best_score:\n",
    "                    best_score = prob\n",
    "                    best_tag = tag\n",
    "\n",
    "            f_out.write(f\"{word} {best_tag if best_tag else 'O'}\\n\")\n",
    "\n",
    "# example usage\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = \"EN/train\"\n",
    "    dev_path = \"EN/dev.in\"\n",
    "    output_path = \"EN/dev.p1.out\"\n",
    "\n",
    "    data = read_training_data(train_path)\n",
    "    word_counts = count_word_frequencies(data)\n",
    "    smoothed_data = replace_rare_words(data, word_counts, k=3)\n",
    "\n",
    "    emission_probs = estimate_emission_parameters(smoothed_data)\n",
    "    known_words = set(word for word, _ in smoothed_data)\n",
    "\n",
    "    baseline_tagger(dev_path, output_path, emission_probs, known_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above with the eval script, we get the following scores:\n",
    "\n",
    "#Entity in gold data: 13179 \n",
    "#Entity in prediction: 19406\n",
    "\n",
    "#Correct Entity : 9152      \n",
    "Entity  precision: 0.4716   \n",
    "Entity  recall: 0.6944      \n",
    "Entity  F: 0.5617\n",
    "\n",
    "#Correct Sentiment : 7644   \n",
    "Sentiment  precision: 0.3939\n",
    "Sentiment  recall: 0.5800   \n",
    "Sentiment  F: 0.4692\n",
    "\n",
    "For a baseline system that is only using emission probabilities, this is not bad. System currently lacks transition modelling and sequence structure!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
